{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846a4484",
   "metadata": {},
   "source": [
    "Question 1: Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52793890",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) is a broad field that focuses on creating systems capable of performing tasks that typically require human intelligence.\n",
    "Machine Learning (ML) is a subset of AI where algorithms learn patterns from data to make predictions or decisions without being explicitly programmed.\n",
    "Deep Learning (DL) is a subset of ML that uses neural networks with many layers to model complex patterns in large datasets.\n",
    "Data Science (DS) is the interdisciplinary field focused on extracting insights from data using statistics, ML, and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246636b0",
   "metadata": {},
   "source": [
    "Question 2: What are the types of machine learning? Describe each with one real-world example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3cdd4",
   "metadata": {},
   "source": [
    "1. Supervised Learning: The model is trained on labeled data. Example: Predicting house prices using historical data.\n",
    "2. Unsupervised Learning: The model finds patterns in unlabeled data. Example: Customer segmentation using clustering.\n",
    "3. Reinforcement Learning: The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties. Example: Training robots to walk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a5027",
   "metadata": {},
   "source": [
    "Question 3: Define overfitting, underfitting, and the bias-variance tradeoff in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050cd79",
   "metadata": {},
   "source": [
    "Overfitting: When a model learns noise and details in the training data so well that it performs poorly on new data.\n",
    "Underfitting: When a model is too simple and fails to capture patterns in the data.\n",
    "Bias-Variance Tradeoff: The balance between bias (error from overly simplistic models) and variance (error from overly complex models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf597a7",
   "metadata": {},
   "source": [
    "Question 4: What are outliers in a dataset, and list three common techniques for handling them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc38b8f",
   "metadata": {},
   "source": [
    "Outliers are data points that significantly deviate from the majority of observations.\n",
    "Techniques:\n",
    "1. Removal of outliers if they are due to data entry errors.\n",
    "2. Transformation (e.g., log transformation) to reduce impact.\n",
    "3. Capping or Winsorization to limit extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefebb9",
   "metadata": {},
   "source": [
    "Question 5: Explain the process of handling missing values and mention one imputation technique for numerical and one for categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9977922",
   "metadata": {},
   "source": [
    "Handling missing values involves identifying, understanding the reason for missingness, and deciding whether to remove or impute them.\n",
    "Numerical imputation: Replace missing values with the mean.\n",
    "Categorical imputation: Replace missing values with the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccba95f",
   "metadata": {},
   "source": [
    "Question 6: Write a Python program that creates a synthetic imbalanced dataset and prints the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db635f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2,\n",
    "                           weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf8f95",
   "metadata": {},
   "source": [
    "Question 7: Implement one-hot encoding using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
    "df = pd.DataFrame({'Color': colors})\n",
    "encoded_df = pd.get_dummies(df, columns=['Color'])\n",
    "print(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886d4a3",
   "metadata": {},
   "source": [
    "Question 8: Generate samples, introduce missing values, impute, and plot histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca100b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.random.normal(50, 10, 1000)\n",
    "missing_indices = np.random.choice(1000, 50, replace=False)\n",
    "data_with_nan = data.copy()\n",
    "data_with_nan[missing_indices] = np.nan\n",
    "\n",
    "# Plot before imputation\n",
    "plt.hist(data_with_nan[~np.isnan(data_with_nan)], bins=30, edgecolor='black')\n",
    "plt.title('Before Imputation')\n",
    "plt.show()\n",
    "\n",
    "# Impute with mean\n",
    "mean_val = np.nanmean(data_with_nan)\n",
    "data_with_nan[np.isnan(data_with_nan)] = mean_val\n",
    "\n",
    "# Plot after imputation\n",
    "plt.hist(data_with_nan, bins=30, edgecolor='black')\n",
    "plt.title('After Imputation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecde35d",
   "metadata": {},
   "source": [
    "Question 9: Implement Min-Max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[2], [5], [10], [15], [20]])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407cb421",
   "metadata": {},
   "source": [
    "Question 10: Data preparation plan for customer transaction dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af051f",
   "metadata": {},
   "source": [
    "Step 1: Handle missing ages → Impute with median (numerical).\n",
    "Step 2: Handle outliers in transaction amount → Use IQR method to detect and cap values.\n",
    "Step 3: Handle imbalance → Apply SMOTE or class weighting.\n",
    "Step 4: Encode categorical variables → Use one-hot encoding for nominal features.\n",
    "Step 5: Scale numerical features → Apply standard scaling or min-max scaling."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
